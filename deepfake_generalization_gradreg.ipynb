{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OYVZT7YW1CW"
      },
      "source": [
        "# Computer Vision 2025 Final Project\n",
        "## Project 2: **Improving Robustness of Deepfake Detectors through Gradient Regularization**\n",
        "\n",
        "Najeh Alhalawani (2223737)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb6YUgKM5H_i"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5352QNWXuuf3",
        "outputId": "82f0cf46-e5e3-4e97-aae8-cf1fc66111ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hud5kplzB1Kv",
        "outputId": "965d24b6-9a10-4147-87b8-fb36efec1260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn) (1.5.1)\n",
            "Collecting lz4>=4.3.3 (from mtcnn)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4, mtcnn\n",
            "Successfully installed lz4-4.4.4 mtcnn-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhRkw4En1kRm",
        "outputId": "b2de0648-26f1-40d1-883a-6ed37cce73b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvYca-oV1l4L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, ConfusionMatrixDisplay, accuracy_score, log_loss, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import cv2\n",
        "from mtcnn import MTCNN\n",
        "import glob\n",
        "import logging\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from random import random\n",
        "import torchvision.transforms.functional as TF\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2bPw-5W4hJe"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NX15ii07wrD"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/dataset_combined'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjsGOQfMuf5-"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('./final.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2TAq6B5UBI"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOxLMPeaBB4a"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK5iQoIm4kGq"
      },
      "outputs": [],
      "source": [
        "folders = ['results', 'baseline', 'grad_reg']\n",
        "\n",
        "for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        print(f\"Created folder: {folder}\")\n",
        "    else:\n",
        "        print(f\"Folder already exists: {folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izw5hN79yC_9"
      },
      "outputs": [],
      "source": [
        "class DataHandler:\n",
        "    def __init__(self, data_dir, batch_size=32, img_size=224):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def load_data(self):\n",
        "        train_dataset = datasets.ImageFolder(os.path.join(self.data_dir, 'train'), transform=self.transform)\n",
        "        val_dataset = datasets.ImageFolder(os.path.join(self.data_dir, 'validation'), transform=self.transform)\n",
        "        test_dataset = datasets.ImageFolder(os.path.join(self.data_dir, 'test'), transform=self.transform)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        self.dataset_stats = {\n",
        "            'train': self.get_dataset_stats(train_dataset, 'train'),\n",
        "            'validation': self.get_dataset_stats(val_dataset, 'validation'),\n",
        "            'test': self.get_dataset_stats(test_dataset, 'test')\n",
        "        }\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    def get_dataset_stats(self, dataset, split_name):\n",
        "        labels = [label for _, label in dataset.samples]\n",
        "        class_counts = Counter(labels)\n",
        "        class_names = dataset.classes\n",
        "        stats = {\n",
        "            'total_samples': len(labels),\n",
        "            'class_distribution': {class_names[i]: count for i, count in class_counts.items()}\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def print_dataset_stats(self):\n",
        "        print(\"\\nDataset Statistics:\")\n",
        "        print(\"=\"*50)\n",
        "        for split, stats in self.dataset_stats.items():\n",
        "            print(f\"\\n{split.capitalize()} Set:\")\n",
        "            print(f\"Total samples: {stats['total_samples']}\")\n",
        "            for class_name, count in stats['class_distribution'].items():\n",
        "                print(f\"{class_name}: {count} samples\")\n",
        "            if len(stats['class_distribution']) == 2:\n",
        "                fake_count = stats['class_distribution'].get('fake', 0)\n",
        "                real_count = stats['class_distribution'].get('real', 0)\n",
        "                print(f\"Fake/Real Ratio: {fake_count/(fake_count + real_count)*100:.2f}% / {real_count/(fake_count + real_count)*100:.2f}%\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "\n",
        "    def augment_real_images_offline(self, num_augments_per_image=2):\n",
        "        real_dir = os.path.join(self.data_dir, 'train', 'real')\n",
        "        image_paths = glob.glob(os.path.join(real_dir, '*.jpg')) + glob.glob(os.path.join(real_dir, '*.png'))\n",
        "\n",
        "        print(f\"\\nAugmenting {len(image_paths)} real images × {num_augments_per_image} times each...\")\n",
        "\n",
        "        count = 0\n",
        "        for path in image_paths:\n",
        "            image = Image.open(path).convert(\"RGB\")\n",
        "            for i in range(num_augments_per_image):\n",
        "                aug_image = self.random_augmentation(image)\n",
        "                aug_filename = f\"aug_{count:05d}.jpg\"\n",
        "                aug_path = os.path.join(real_dir, aug_filename)\n",
        "                aug_image.save(aug_path)\n",
        "                count += 1\n",
        "\n",
        "        print(f\"Saved {count} augmented images to: {real_dir}\")\n",
        "\n",
        "    def random_augmentation(self, image):\n",
        "        # Define random transform manually\n",
        "        image = image.resize((self.img_size, self.img_size))\n",
        "\n",
        "        if random.random() > 0.5:\n",
        "            image = TF.hflip(image)\n",
        "        if random.random() > 0.5:\n",
        "            angle = random.uniform(-15, 15)\n",
        "            image = TF.rotate(image, angle)\n",
        "        if random.random() > 0.5:\n",
        "            brightness = random.uniform(0.8, 1.2)\n",
        "            image = TF.adjust_brightness(image, brightness)\n",
        "        if random.random() > 0.5:\n",
        "            contrast = random.uniform(0.8, 1.2)\n",
        "            image = TF.adjust_contrast(image, contrast)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT1Q2y0kuuRT"
      },
      "outputs": [],
      "source": [
        "### Extract frames from FaceForensics+++\n",
        "def preprocess_frame(frame):\n",
        "    # convert to grayscale and apply histogram equalization to improve contrast\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    equalized_frame = cv2.equalizeHist(gray_frame)\n",
        "    # convert back to RGB for MTCNN\n",
        "    rgb_frame = cv2.cvtColor(equalized_frame, cv2.COLOR_GRAY2RGB)\n",
        "    return rgb_frame\n",
        "\n",
        "def process_video(video_path, output_full_dir, output_faces_dir):\n",
        "    os.makedirs(output_full_dir, exist_ok=True)\n",
        "    os.makedirs(output_faces_dir, exist_ok=True)\n",
        "    video_name = Path(video_path).stem\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: could not open video {video_path}\")\n",
        "        return False\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total_frames <= 0:\n",
        "        print(f\"Error: invalid frame count for {video_path}\")\n",
        "        cap.release()\n",
        "        return False\n",
        "\n",
        "    frame_positions = [\n",
        "        int(total_frames * 0.2),  # 20% of video\n",
        "        int(total_frames * 0.8)   # 80% of video\n",
        "    ]\n",
        "\n",
        "    detector = MTCNN()\n",
        "\n",
        "    for idx, frame_pos in enumerate(frame_positions, 1):\n",
        "        # set to the specified frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"Error: could not read frame at position {frame_pos} in {video_path}\")\n",
        "            continue\n",
        "\n",
        "        # save full frame\n",
        "        full_frame_path = os.path.join(output_full_dir, f\"{video_name}_full_{idx*20}.jpg\")\n",
        "        cv2.imwrite(full_frame_path, frame)\n",
        "        print(f\"Saved full frame: {full_frame_path}\")\n",
        "\n",
        "        # preprocess frame for better detection\n",
        "        rgb_frame = preprocess_frame(frame)\n",
        "\n",
        "        # detect faces\n",
        "        faces = detector.detect_faces(rgb_frame)\n",
        "\n",
        "        if faces:\n",
        "            for i, face in enumerate(faces):\n",
        "                x, y, w, h = face['box']\n",
        "                # ensure bounding box is valid\n",
        "                if w <= 0 or h <= 0:\n",
        "                    print(f\"Invalid bounding box for face {i+1} at frame {frame_pos} in {video_path}\")\n",
        "                    continue\n",
        "\n",
        "                # extract face region with padding\n",
        "                padding = 20\n",
        "                top = max(0, y - padding)\n",
        "                left = max(0, x - padding)\n",
        "                bottom = min(frame.shape[0], y + h + padding)\n",
        "                right = min(frame.shape[1], x + w + padding)\n",
        "\n",
        "                # crop face from original frame (not preprocessed, to preserve color)\n",
        "                face_frame = frame[top:bottom, left:right]\n",
        "\n",
        "                # check if face frame is valid\n",
        "                if face_frame.size == 0:\n",
        "                    print(f\"Empty face frame for face {i+1} at frame {frame_pos} in {video_path}\")\n",
        "                    continue\n",
        "\n",
        "                # resize face to 224x224\n",
        "                try:\n",
        "                    face_frame_resized = cv2.resize(face_frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error resizing face {i+1} at frame {frame_pos} in {video_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                face_frame_path = os.path.join(output_faces_dir, f\"{video_name}_face_{idx*20}_{i+1}.jpg\")\n",
        "                cv2.imwrite(face_frame_path, face_frame_resized)\n",
        "                print(f\"Saved face frame: {face_frame_path} (Face {i+1}, Confidence: {face['confidence']:.2f})\")\n",
        "        else:\n",
        "            print(f\"No faces detected at frame {frame_pos} in {video_path}\")\n",
        "\n",
        "    cap.release()\n",
        "    return True\n",
        "\n",
        "def process_videos_in_directory(video_dir, output_full_dir, output_faces_dir, start_from=0):\n",
        "    video_extensions = ('.mp4', '.avi', '.mov', '.mkv')\n",
        "    video_list = sorted([os.path.join(video_dir, f) for f in os.listdir(video_dir)\n",
        "                        if os.path.isfile(os.path.join(video_dir, f)) and f.lower().endswith(video_extensions)])\n",
        "\n",
        "    if not video_list:\n",
        "        print(f\"No videos found in {video_dir}\")\n",
        "        return\n",
        "\n",
        "    if start_from < 0 or start_from >= len(video_list):\n",
        "        print(f\"Error: start_from index {start_from} is out of range. Valid range: 0 to {len(video_list)-1}\")\n",
        "        return\n",
        "\n",
        "    for idx, video_path in enumerate(video_list[start_from:], start=start_from):\n",
        "        print(f\"\\nProcessing video {idx+1}/{len(video_list)}: {video_path}\")\n",
        "        success = process_video(video_path, output_full_dir, output_faces_dir)\n",
        "        if not success:\n",
        "            print(f\"Failed to process {video_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoQYEe4V5j5E"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6XKkV07tbZm"
      },
      "source": [
        "### Download DFFD Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCYwHvRXVfr0"
      },
      "source": [
        "Retrieve the password from the official website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z0eDwcqtaPV"
      },
      "outputs": [],
      "source": [
        "!wget https://www.cse.msu.edu/computervision/dffd_dataset/stylegan_celeba.zip --no-check-certificate  -P DFFD\n",
        "!wget https://www.cse.msu.edu/computervision/dffd_dataset/stargan.zip --no-check-certificate  -P DFFD\n",
        "!wget https://www.cse.msu.edu/computervision/dffd_dataset/stylegan_ffhq.zip --no-check-certificate  -P DFFD\n",
        "!wget https://www.cse.msu.edu/computervision/dffd_dataset/pggan_v1.zip --no-check-certificate  -P DFFD\n",
        "!wget https://www.cse.msu.edu/computervision/dffd_dataset/pggan_v2.zip --no-check-certificate  -P DFFD\n",
        "!wget https://www.cse.msu.edu/computervision/dffd_dataset/ffhq.zip --no-check-certificate  -P DFFD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI6w9c7rth5o"
      },
      "outputs": [],
      "source": [
        "!unzip  ./DFFD/stargan.zip -d ./content/\n",
        "!unzip  ./DFFD/stylegan_celeba.zip -d ./content/\n",
        "!unzip  ./DFFD/stylegan_ffhq.zip -d ./content/\n",
        "!unzip  ./DFFD/pggan_v1.zip -d ./content/\n",
        "!unzip  ./DFFD/pggan_v2.zip -d ./content/\n",
        "!unzip  ./DFFD/ffhq.zip -d ./content/real/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLi3I-3ltneX"
      },
      "source": [
        "### Split dataset into fake/real inside train/test/validation folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxRJ6E9Otqcc"
      },
      "outputs": [],
      "source": [
        "base_dir = './content'\n",
        "\n",
        "output_base = os.path.join(base_dir, 'dataset_combined')\n",
        "splits = ['train', 'test', 'validation']\n",
        "classes = ['fake', 'real']\n",
        "\n",
        "for split in splits:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(output_base, split, cls), exist_ok=True)\n",
        "\n",
        "for folder_name in os.listdir(base_dir):\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "\n",
        "    if folder_name == 'dataset_combined' or not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    for split in splits:\n",
        "        source_split_path = os.path.join(folder_path, split)\n",
        "\n",
        "        if os.path.exists(source_split_path):\n",
        "            for file_name in os.listdir(source_split_path):\n",
        "                src_file = os.path.join(source_split_path, file_name)\n",
        "\n",
        "                # Determine destination class: real or fake\n",
        "                if file_name.startswith('R_'):\n",
        "                    cls = 'real'\n",
        "                else:\n",
        "                    cls = 'fake'\n",
        "\n",
        "                dest_path = os.path.join(output_base, split, cls)\n",
        "                dst_file = os.path.join(dest_path, file_name)\n",
        "\n",
        "                if os.path.exists(dst_file):\n",
        "                    base, ext = os.path.splitext(file_name)\n",
        "                    count = 1\n",
        "                    while os.path.exists(dst_file):\n",
        "                        new_name = f\"{base}_{count}{ext}\"\n",
        "                        dst_file = os.path.join(dest_path, new_name)\n",
        "                        count += 1\n",
        "\n",
        "                shutil.copy2(src_file, dst_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M19NYfKeVaY2"
      },
      "source": [
        "### Analyze Fake/Real Ratio in DFFD Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gut2kOKPSUGS",
        "outputId": "ff7a435a-df69-47cb-9865-fc23fbf8180a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Statistics:\n",
            "==================================================\n",
            "\n",
            "Train Set:\n",
            "Total samples: 59956\n",
            "fake: 49956 samples\n",
            "real: 10000 samples\n",
            "Fake/Real Ratio: 83.32% / 16.68%\n",
            "\n",
            "Validation Set:\n",
            "Total samples: 5997\n",
            "fake: 4998 samples\n",
            "real: 999 samples\n",
            "Fake/Real Ratio: 83.34% / 16.66%\n",
            "\n",
            "Test Set:\n",
            "Total samples: 66860\n",
            "fake: 57860 samples\n",
            "real: 9000 samples\n",
            "Fake/Real Ratio: 86.54% / 13.46%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "data_handler = DataHandler('./content/dataset_combined/')\n",
        "train_loader, val_loader, test_loader = data_handler.load_data()\n",
        "data_handler.print_dataset_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siqE8FpPPm1q"
      },
      "source": [
        "### FaceForencics++ Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY1HENnTvWHx"
      },
      "outputs": [],
      "source": [
        "!python download.py -c c23 --server EU2 -d original ./real_videos/Original_Videos_FF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOPftExVwrkj"
      },
      "source": [
        "* Extract 4 Frames From Each Video\n",
        "* Face Detection using MTCNN\n",
        "* Resize Image To The Bounding Box of The Face\n",
        "* Save Detected Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPGqyKkowOEq"
      },
      "outputs": [],
      "source": [
        "video_directory = \"./real_videos/Original_Videos_FF/original_sequences/youtube/c23/videos\"\n",
        "output_full_directory = \"./real_videos/full/\"\n",
        "output_faces_directory = \"./real_videos/faces/\"\n",
        "\n",
        "\n",
        "process_videos_in_directory(video_directory, output_full_directory, output_faces_directory, start_from=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKA4W3E-Qrii"
      },
      "source": [
        "### Append Newly Extracted Faces Into The Combined Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWGJbmqKQxt4",
        "outputId": "e817f990-ff18-4a3a-e23a-888331750cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Copied 1614 to train/real/\n",
            "✅ Copied 346 to validation/real/\n",
            "✅ Copied 347 to test/real/\n"
          ]
        }
      ],
      "source": [
        "random.seed(42)\n",
        "\n",
        "new_real_dir = Path(\"./real_videos/faces/\")\n",
        "dataset_root = Path(\"./content/dataset_combined/\")\n",
        "\n",
        "all_real_samples = list(new_real_dir.glob(\"*\"))\n",
        "random.shuffle(all_real_samples)\n",
        "\n",
        "total = len(all_real_samples)\n",
        "train_split = int(0.7 * total)\n",
        "val_split = int(0.15 * total)\n",
        "test_split = total - train_split - val_split\n",
        "\n",
        "train_samples = all_real_samples[:train_split]\n",
        "val_samples = all_real_samples[train_split:train_split + val_split]\n",
        "test_samples = all_real_samples[train_split + val_split:]\n",
        "\n",
        "def copy_samples(samples, dest_dir):\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for sample in samples:\n",
        "        shutil.copy(sample, dest_dir / sample.name)\n",
        "\n",
        "copy_samples(train_samples, dataset_root / \"train\" / \"real\")\n",
        "copy_samples(val_samples, dataset_root / \"validation\" / \"real\")\n",
        "copy_samples(test_samples, dataset_root / \"test\" / \"real\")\n",
        "\n",
        "print(f\"✅ Copied {len(train_samples)} to train/real/\")\n",
        "print(f\"✅ Copied {len(val_samples)} to validation/real/\")\n",
        "print(f\"✅ Copied {len(test_samples)} to test/real/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYMjYd5jVy_Z"
      },
      "source": [
        "### Analyze Fake/Real Ratio After Appending FaceForensics++ Real Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RNfAqMsVxhn",
        "outputId": "07b074bc-563c-4ebf-cc54-4ed7c9074596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Statistics:\n",
            "==================================================\n",
            "\n",
            "Train Set:\n",
            "Total samples: 61570\n",
            "fake: 49956 samples\n",
            "real: 11614 samples\n",
            "Fake/Real Ratio: 81.14% / 18.86%\n",
            "\n",
            "Validation Set:\n",
            "Total samples: 6343\n",
            "fake: 4998 samples\n",
            "real: 1345 samples\n",
            "Fake/Real Ratio: 78.80% / 21.20%\n",
            "\n",
            "Test Set:\n",
            "Total samples: 67207\n",
            "fake: 57860 samples\n",
            "real: 9347 samples\n",
            "Fake/Real Ratio: 86.09% / 13.91%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "data_handler = DataHandler('./content/dataset_combined/')\n",
        "train_loader, val_loader, test_loader = data_handler.load_data()\n",
        "data_handler.print_dataset_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN9Ib804zYM0"
      },
      "source": [
        "### Augment Real Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdygsFxuzaqs",
        "outputId": "ba3e4fe5-e9df-4ebf-aad4-872c9df3869b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Augmenting 11614 real images × 3 times each...\n",
            "Saved 34842 augmented images to: ./content/dataset_combined/train/real\n",
            "\n",
            "Dataset Statistics:\n",
            "==================================================\n",
            "\n",
            "Train Set:\n",
            "Total samples: 96412\n",
            "fake: 49956 samples\n",
            "real: 46456 samples\n",
            "Fake/Real Ratio: 51.82% / 48.18%\n",
            "\n",
            "Validation Set:\n",
            "Total samples: 6343\n",
            "fake: 4998 samples\n",
            "real: 1345 samples\n",
            "Fake/Real Ratio: 78.80% / 21.20%\n",
            "\n",
            "Test Set:\n",
            "Total samples: 67207\n",
            "fake: 57860 samples\n",
            "real: 9347 samples\n",
            "Fake/Real Ratio: 86.09% / 13.91%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "handler = DataHandler(data_dir='./content/dataset_combined/')\n",
        "handler.augment_real_images_offline(num_augments_per_image=3)\n",
        "\n",
        "data_handler = DataHandler('./content/dataset_combined/')\n",
        "train_loader, val_loader, test_loader = data_handler.load_data()\n",
        "data_handler.print_dataset_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVG8nFOi6Dbe"
      },
      "source": [
        "### Using the combined dataset (dataset_combined_v2.zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzqme-Fy5zrw"
      },
      "outputs": [],
      "source": [
        "!unzip /data/dataset_combined_v2.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCKtniLM5zrw"
      },
      "outputs": [],
      "source": [
        "!mv /content/content/dataset_combined /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5L-RLkt5zrw"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load The Combined Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXH6oipW5741"
      },
      "outputs": [],
      "source": [
        "# for both baseline and gradient regularization models\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'validation'), transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhLCflR28fxo"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vF1JP4dutN_",
        "outputId": "71083945-760e-4eaf-9211-f3dcf78b7f39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 205MB/s]\n"
          ]
        }
      ],
      "source": [
        "### BASELINE MODEL\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNYAbLi5Vfr3"
      },
      "source": [
        "Equation 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pely1FdMVfr3"
      },
      "source": [
        "$$\n",
        "f'_{\\theta_{s}}(x) = f_{\\theta_{s}}(x)_{\\text{norm}}(\\sigma_{s} + \\Delta\\sigma_{s}) + (\\mu_{s} + \\Delta\\mu_{s})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lexFFyLA82eJ"
      },
      "outputs": [],
      "source": [
        "### GRADIENT REGULARIZATION MODEL\n",
        "\n",
        "\"\"\"\n",
        " The Perturbation Injection Module applies the calculated adversarial perturbation\n",
        " to the shallow feature statistics, It is the mechanism for creating the perturbed features used in the second pass.\n",
        "\"\"\"\n",
        "class PerturbationInjectionModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies perturbations to the shallow feature statistics.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(PerturbationInjectionModule, self).__init__()\n",
        "\n",
        "    def forward(self, x_norm, mu_s, sigma_s, delta_mu_s, delta_sigma_s):\n",
        "        # Denormalize the features with perturbed statistics (mean and standard deviation) as per Equation (7)\n",
        "        x_perturbed = x_norm * (sigma_s + delta_sigma_s) + (mu_s + delta_mu_s)\n",
        "        return x_perturbed\n",
        "\"\"\"\n",
        " Defines the main model architecture, explicitly separating it into shallow and deep parts.\n",
        " This separation is crucial for isolating the layers where texture patterns are captured and perturbations are applied.\n",
        "\"\"\"\n",
        "class DeepfakeDetector(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Deepfake Detector model, wrapping a backbone using EfficientNet.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(DeepfakeDetector, self).__init__()\n",
        "        base_model = models.efficientnet_b0(pretrained=pretrained)\n",
        "        # The first few layers capture low-level texture features, these will be perturbed.\n",
        "        self.shallow_features_extractor = nn.Sequential(*list(base_model.features[:2]))\n",
        "        # The remaining layers process higher-level features.\n",
        "        self.deep_features_extractor = nn.Sequential(*list(base_model.features[2:]))\n",
        "        self.avgpool = base_model.avgpool\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.2, inplace=True),\n",
        "            nn.Linear(base_model.classifier[1].in_features, 2)  # 2 classes: real/fake\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shallow_features_extractor(x)\n",
        "        x = self.deep_features_extractor(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward_from_shallow(self, shallow_out):\n",
        "        \"\"\"\n",
        "        A forward pass starting from the output of the shallow layers.\n",
        "        This is essential for the two-pass algorithm, as it allows us to process both original and perturbed shalllow features\n",
        "        without re-computing the initial layers.\n",
        "        \"\"\"\n",
        "        x = self.deep_features_extractor(shallow_out)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def compute_statistics(x):\n",
        "    \"\"\"\n",
        "    Computes channel-wise mean and standard deviation for shallow features.\n",
        "    These statistics are used to represent the image texture patterns.\n",
        "    \"\"\"\n",
        "    mu_s = x.mean(dim=[2, 3], keepdim=True)\n",
        "    sigma_s = torch.sqrt(x.var(dim=[2, 3], keepdim=True) + 1e-8)  # Add epsilon for stability\n",
        "    return mu_s, sigma_s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjiqGK4q42zP"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv-GRhwZutAf"
      },
      "outputs": [],
      "source": [
        "### BASELINE MODEL\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for imgs, labels in tqdm(loader, desc=\"training\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    return total_loss / len(loader), correct / len(loader.dataset)\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss, train_acc = train(model, train_loader)\n",
        "\n",
        "    print(f\"epoch {epoch+1}:\")\n",
        "    print(f\"train loss = {train_loss:.4f}, train accuracy = {train_acc:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"baseline/baseline_model_5_combined.pth\")  # saving model weights only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXPyl01xVfr4"
      },
      "source": [
        "### Implementation Pipeline\n",
        "1. Initialization\n",
        "   * For each batch of data, start by cleaning any existing gradients from the optimizer.\n",
        "2. Pass 1: Gradient from Original Data (g1)\n",
        "   * Perform a forward pass using the original, unperturbed input data to calculate the standard empirial loss, L_original\n",
        "   * Perform a backward pass on L_original. This computes two things:\n",
        "      * The gradients of the loss with respect to the model's parameters, which are stored as g1.\n",
        "      * The gradients of the loss with respect to the shallow feature statistics\n",
        "3. Perturbation Calculation\n",
        "   * Use the gradients of the feature statistics mean, std from the previous step to calculate the perturbation vector. This is done by taking a small step of size r in the direction of this gradient as defined in equation (9)\n",
        "4. Pass 2: Gradient from Perturbed Data (g2)\n",
        "   * Apply the calculated perturbation to the shallow features of the original input data using the PIM.\n",
        "   * Perform a forward pass using these new, perturbed features to calculate the perturbed loss L_perturbed.\n",
        "   * Perform a second backward pass on L_perturbed to compute the gradients of the model's parameters, which are stored as (g2)\n",
        "5. Combine Gradients and update\n",
        "   * Manually set the final gradient for each model parameter by combining the stored gradients.\n",
        "   * Call the optimizer to perform a single update step using this final, combined gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjlGI6gXVfr4"
      },
      "source": [
        "Equation 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O66UU8eTVfr4"
      },
      "source": [
        "$$\n",
        "L(x, y, \\theta) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\text{CE}(f_{\\theta_{d}}(f_{\\theta_{s}}(x_{i})), y_{i})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dOVrhtoVfr4"
      },
      "source": [
        "Equation 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iFyPmUgVfr4"
      },
      "source": [
        "$$\n",
        "f_{\\theta_{s}}(x)_{\\text{norm}}=\\frac{f_{\\theta_{s}}(x)-\\mu_{s}}{\\sigma_{s}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtH9zpBTVfr4"
      },
      "source": [
        "Equation 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iC0H1dpVfr4"
      },
      "source": [
        "$$\n",
        "\\Delta l = r \\frac{\\nabla_{\\mu_{s},\\sigma_{s}}L(f_{\\theta_{s}}(x),y,\\theta_{d})}{\\|\\nabla_{\\mu_{s},\\sigma_{s}}L(f_{\\theta_{s}}(x),y,\\theta_{d})\\|_{2}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq6liGxqVfr4"
      },
      "source": [
        "Equation 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ev0fzITVfr4"
      },
      "source": [
        "$$\n",
        "\\min_{\\theta} (1-\\alpha)L(x, y, \\theta) + \\alpha L(f'_{\\theta_{s}}(x), y, \\theta_{d})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_viYLCX9XW0",
        "outputId": "dee8b3ee-22e8-4374-965c-5c3c4019e9da"
      },
      "outputs": [],
      "source": [
        "### GRADIENT REGULARIZATION MODEL\n",
        "R_SCALAR = 0.05     # Approximation scalar 'r' controls the magnitude of the perturbation\n",
        "ALPHA_COEFF = 1.0   # Balance coefficient 'α' balance coefficient for the two loss components\n",
        "EPOCHS = 10         # Number of training epochs\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model = DeepfakeDetector(pretrained=True).to(device)\n",
        "pim = PerturbationInjectionModule().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n",
        "scaler = GradScaler()  # Initialize GradScaler for AMP\n",
        "\n",
        "logging.info(f\"Hyperparameters: epochs={EPOCHS}, lr={LEARNING_RATE}, r={R_SCALAR}, alpha={ALPHA_COEFF}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\")):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        \"\"\" Step 1: First Forward Pass for g1\n",
        "        This pass calculates the standard empirical loss and computes the gradients needed for both\n",
        "        the model update g1 and the perturbation calculation.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():  # Enable mixed precision\n",
        "            # Get shallow features\n",
        "            shallow_out = model.shallow_features_extractor(inputs)\n",
        "\n",
        "            # Compute the feature statistics (μ_s, σ_s) that represent image texture.\n",
        "            mu_s, sigma_s = compute_statistics(shallow_out)\n",
        "\n",
        "            \"\"\" Make sure statistics require gradients\n",
        "            This makes them leaf nodes, allowing us to get their gradients explicitly.\n",
        "            \"\"\"\n",
        "            mu_s = mu_s.detach().requires_grad_(True)\n",
        "            sigma_s = sigma_s.detach().requires_grad_(True)\n",
        "\n",
        "            # First forward pass\n",
        "            # Apply equation (6) to normalize the shallow features\n",
        "            x_norm = (shallow_out - mu_s) / (sigma_s + 1e-8)\n",
        "            reconstructed_shallow = x_norm * sigma_s + mu_s\n",
        "\n",
        "            # Calculate the standard empirical loss on the original unperturbed data\n",
        "            # Equation (1)\n",
        "            outputs_original = model.forward_from_shallow(reconstructed_shallow)\n",
        "            L_original = criterion(outputs_original, labels)\n",
        "\n",
        "        \"\"\"Scale loss and compute gradients\n",
        "        The first backward pass. It computes the gradient of the loss w.r.t all parameters in the graph.\n",
        "        retain_graph=True is crucial as it keeps the computation graph intact for the second backward pass\n",
        "        \"\"\"\n",
        "        scaler.scale(L_original).backward(retain_graph=True)\n",
        "\n",
        "        \"\"\"\n",
        "        Explicitly extract the gradients of the loss w.r.t the statistics (μ_s, σ_s).\n",
        "        These gradients indicate the direction of vulnerability of the model to texture changes.\n",
        "        \"\"\"\n",
        "        grad_mu_s, grad_sigma_s = torch.autograd.grad(\n",
        "            outputs=L_original,\n",
        "            inputs=[mu_s, sigma_s],\n",
        "            retain_graph=True,\n",
        "            create_graph=False\n",
        "        )\n",
        "\n",
        "        # Store gradients g1: the gradients of the model parameters after the first backward pass.\n",
        "        # g1: gradient of the original loss\n",
        "        g1 = [p.grad.clone() if p.grad is not None else torch.zeros_like(p)\n",
        "              for p in model.parameters()]\n",
        "\n",
        "        \"\"\"\n",
        "        This computes the actual perturbation vector based on the gradients found in the previous step.\n",
        "        \"\"\"\n",
        "        grad_vec = torch.cat([grad_mu_s.flatten(), grad_sigma_s.flatten()])\n",
        "        norm_grad = torch.norm(grad_vec)\n",
        "\n",
        "        # Step 2: Compute Perturbations by taking a small step 'r' in the direction of the gradients\n",
        "        # Equation (9)\n",
        "        if norm_grad > 1e-8:\n",
        "            delta_l = R_SCALAR * grad_vec / norm_grad\n",
        "        else:\n",
        "            delta_l = torch.zeros_like(grad_vec)\n",
        "\n",
        "        # Reshape perturbation vector back to the original statistics shape (mean, std)\n",
        "        delta_mu_s = delta_l[:mu_s.numel()].view_as(mu_s)\n",
        "        delta_sigma_s = delta_l[mu_s.numel():].view_as(sigma_s)\n",
        "\n",
        "        \"\"\" Step 3: Second Forward/Backward Pass for g2\n",
        "        This pass calculates the loss and gradient on the perturbed shallow features.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            # normalize the original shallow features\n",
        "            x_norm = (shallow_out.detach() - mu_s.detach()) / (sigma_s.detach() + 1e-8)\n",
        "            # use PIM to create the perturbed shallow features\n",
        "            perturbed_shallow_out = pim(x_norm, mu_s.detach(), sigma_s.detach(), delta_mu_s, delta_sigma_s)\n",
        "\n",
        "            \"\"\"\n",
        "            Calculate the loss on the perturbed data\n",
        "            Loss function from Equation 12\n",
        "            \"\"\"\n",
        "            outputs_perturbed = model.forward_from_shallow(perturbed_shallow_out)\n",
        "            L_perturbed = criterion(outputs_perturbed, labels)\n",
        "\n",
        "        \"\"\" Scale loss and compute gradients\n",
        "        This calculates the gradients of the model parameters w.r.t the perturbed loss.\n",
        "        \"\"\"\n",
        "        scaler.scale(L_perturbed).backward()\n",
        "\n",
        "        # Store gradients g2 from the second pass.\n",
        "        # g2: gradient of the perturbed loss\n",
        "        g2 = [p.grad.clone() if p.grad is not None else torch.zeros_like(p)\n",
        "              for p in model.parameters()]\n",
        "\n",
        "        # Step 4: Combine Gradients and Update Weights\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            for p_idx, p in enumerate(model.parameters()):\n",
        "                \"\"\"\n",
        "                set the final gradient for each parameter using the weighed combination of g1 and g2\n",
        "                this update rule encourages both accuracy (from g1) and robustness (from g2)\n",
        "                \"\"\"\n",
        "                combined_grad = (1 - ALPHA_COEFF) * g1[p_idx] + ALPHA_COEFF * g2[p_idx]\n",
        "                p.grad = combined_grad\n",
        "\n",
        "        # Update weights with scaler\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        \"\"\"\n",
        "        Calculate the combined loss\n",
        "        \"\"\"\n",
        "        combined_loss = (1 - ALPHA_COEFF) * L_original.item() + ALPHA_COEFF * L_perturbed.item()\n",
        "        running_loss += combined_loss\n",
        "\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            logging.info(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], Loss: {combined_loss:.4f}\")\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    logging.info(f\"\\nEpoch {epoch+1}\")\n",
        "    logging.info(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "        save_path = f'grad_reg/model_epoch_{epoch+1}.pth'\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        logging.info(f\"Model saved to '{save_path}'\")\n",
        "\n",
        "torch.save(model.state_dict(), 'grad_reg/final-full.pth')\n",
        "logging.info(\"Final model saved to 'final-full.pth'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A05HEglkxQPy"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQwVWofTAuJV"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### BASELINE MODEL\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load(\"baseline/baseline_model_5_combined.pth\", map_location=torch.device('cpu'))) # loading model weights\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]  # probability for class 1 (deepfake)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    class_names = [\"real\", \"deepfake\"]\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"predicted\")\n",
        "    plt.ylabel(\"true\")\n",
        "    plt.title(\"confusion matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nclassification:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    print(f\"test accuracy: {accuracy:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "4Ss3vtnw7Ijs",
        "outputId": "cf7404c0-6c37-4b14-de14-a60315d4dda4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        real       1.00      1.00      1.00     57860\n",
            "    deepfake       1.00      1.00      1.00      9347\n",
            "\n",
            "    accuracy                           1.00     67207\n",
            "   macro avg       1.00      1.00      1.00     67207\n",
            "weighted avg       1.00      1.00      1.00     67207\n",
            "\n",
            "test accuracy: 0.999464\n"
          ]
        }
      ],
      "source": [
        "evaluate_with_metrics(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Regularization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### GRADIENT REGULARIZATION MODEL\n",
        "def evaluate_model(model, data_loader, criterion, phase='val'):\n",
        "    model.eval()\n",
        "    y_true, y_scores = [], []\n",
        "    total_loss = 0.0\n",
        "    scaler = GradScaler()  # Initialize GradScaler for AMP\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"Evaluating {phase}\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with autocast():  # Enable mixed precision\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_scores.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    logloss = log_loss(y_true, y_scores)\n",
        "    y_pred = np.round(y_scores)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    return avg_loss, auc, logloss, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb-qThUC7PCa",
        "outputId": "27d691c6-6680-494c-be0a-b79cc203755e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-1006374996>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Initialize GradScaler for AMP\n",
            "Evaluating test:   0%|          | 0/2101 [00:00<?, ?it/s]<ipython-input-15-1006374996>:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Enable mixed precision\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.0029\n",
            "Test AUC: 1.0000\n",
            "Test LogLoss: 0.0029\n",
            "Test F1-Score: 0.9968\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model = DeepfakeDetector(pretrained=False).to(device)\n",
        "model.load_state_dict(torch.load(\"grad_reg/model_epoch_6.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "avg_test_loss, test_auc, test_logloss, test_f1 = evaluate_model(model, test_loader, criterion, phase='test')\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test LogLoss: {test_logloss:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVd3Cgdy4xx8"
      },
      "source": [
        "## Adversarial Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S85zwO6v-zmQ"
      },
      "outputs": [],
      "source": [
        "def fgsm_attack(image, label, model, epsilon):\n",
        "    image = image.clone().detach().to(device).requires_grad_(True)\n",
        "    label = label.to(device)\n",
        "    output = model(image)\n",
        "    loss = F.cross_entropy(output, label)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    perturbed_image = image + epsilon * image.grad.sign()\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    return perturbed_image.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XOiIxKz_NwD"
      },
      "outputs": [],
      "source": [
        "def pgd_attack(model, images, labels, epsilon, alpha, iters):\n",
        "    images = images.clone().detach().to(device)\n",
        "    labels = labels.to(device)\n",
        "    ori_images = images.clone().detach()\n",
        "\n",
        "    for i in range(iters):\n",
        "        images.requires_grad_(True)\n",
        "        outputs = model(images)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Step in the direction of the gradient\n",
        "        adv_images = images + alpha * images.grad.sign()\n",
        "\n",
        "        # Project back to epsilon-ball and clip pixel values\n",
        "        eta = torch.clamp(adv_images - ori_images, min=-epsilon, max=epsilon)\n",
        "        images = torch.clamp(ori_images + eta, 0, 1).detach()\n",
        "\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6vXdm7bABfF"
      },
      "outputs": [],
      "source": [
        "def evaluate_adversarial(model, loader, attack_type, epsilon, pgd_steps=None, alpha=None, save_path=None):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "\n",
        "    desc = f\"{attack_type} epsilon={epsilon:.4f}\"\n",
        "    loop = tqdm(loader, desc=desc, leave=False)\n",
        "\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if attack_type == \"FGSM\":\n",
        "            adv_images = fgsm_attack(images, labels, model, epsilon)\n",
        "        elif attack_type == \"PGD\":\n",
        "            adv_images = pgd_attack(model, images, labels, epsilon, alpha, pgd_steps)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown attack type\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(adv_images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            probs = F.softmax(outputs, dim=1)[:, 1]  # Probability of class 1\n",
        "\n",
        "        correct += torch.sum(preds == labels).item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "        loop.set_postfix(acc=correct / total)\n",
        "\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "\n",
        "    acc = correct / total\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "    auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.0\n",
        "\n",
        "    print(f\"{attack_type} epsilon: {epsilon:.4f}, accuracy: {acc:.4f}, F1 Score: {f1:.4f}, AUC Score: {auc:.4f}\")\n",
        "\n",
        "    if save_path:\n",
        "        torch.save({\n",
        "            'epsilon': epsilon,\n",
        "            'labels': torch.tensor(all_labels),\n",
        "            'preds': torch.tensor(all_preds),\n",
        "            'probs': torch.tensor(all_probs),\n",
        "        }, save_path)\n",
        "        print(f\"Saved to: {save_path}\")\n",
        "\n",
        "    return acc, f1, auc, all_labels, all_preds, all_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHnONI5GBRoS"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_attacks(checkpoint_path, model_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if \"baseline\" in checkpoint_path:\n",
        "        model = models.efficientnet_b0(pretrained=True)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n",
        "    else:\n",
        "        model = DeepfakeDetector(pretrained=True)\n",
        "\n",
        "\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    fgsm_epsilons = [0.001, 0.005, 0.01, 0.02]\n",
        "    pgd_epsilons = [0.0001, 0.0005, 0.001]\n",
        "    pgd_steps = 5\n",
        "\n",
        "    print(f\"\\nEvaluating attacks on {model_name}\")\n",
        "\n",
        "    for eps in fgsm_epsilons:\n",
        "        save_path = f\"{model_name}_fgsm_eps_{eps:.4f}.pth\"\n",
        "        evaluate_adversarial(model, test_loader, \"FGSM\", eps, save_path=save_path)\n",
        "\n",
        "    for eps in pgd_epsilons:\n",
        "        alpha = eps / pgd_steps\n",
        "        save_path = f\"{model_name}_pgd_eps_{eps:.4f}.pth\"\n",
        "        evaluate_adversarial(model, test_loader, \"PGD\", eps, pgd_steps=pgd_steps, alpha=alpha, save_path=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_model_attacks(\"baseline/baseline_model_5_combined.pth\", \"baseline\") # baseline model\n",
        "evaluate_model_attacks(\"grad_reg/model_epoch_6.pth\", \"grad_reg\") # model with gradient regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ablation Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perturbation Layers\n",
        "Perturbation on the first three layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepfakeDetector_3(nn.Module):\n",
        "    \"\"\"\n",
        "    The main Deepfake Detector model, wrapping a backbone using EfficientNet.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(DeepfakeDetector_3, self).__init__()\n",
        "        base_model = models.efficientnet_b0(pretrained=pretrained)\n",
        "        # The first few layers capture low-level texture features, these will be perturbed.\n",
        "        self.shallow_features_extractor = nn.Sequential(*list(base_model.features[:3])) # First three layers\n",
        "        # The remaining layers process higher-level features.\n",
        "        self.deep_features_extractor = nn.Sequential(*list(base_model.features[3:])) # First three layers\n",
        "        self.avgpool = base_model.avgpool\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.2, inplace=True),\n",
        "            nn.Linear(base_model.classifier[1].in_features, 2)  # 2 classes: real/fake\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shallow_features_extractor(x)\n",
        "        x = self.deep_features_extractor(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward_from_shallow(self, shallow_out):\n",
        "        \"\"\"\n",
        "        A forward pass starting from the output of the shallow layers.\n",
        "        This is essential for the two-pass algorithm, as it allows us to process both original and perturbed shalllow features\n",
        "        without re-computing the initial layers.\n",
        "        \"\"\"\n",
        "        x = self.deep_features_extractor(shallow_out)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def compute_statistics(x):\n",
        "    \"\"\"\n",
        "    Computes channel-wise mean and standard deviation for shallow features.\n",
        "    These statistics are used to represent the image texture patterns.\n",
        "    \"\"\"\n",
        "    mu_s = x.mean(dim=[2, 3], keepdim=True)\n",
        "    sigma_s = torch.sqrt(x.var(dim=[2, 3], keepdim=True) + 1e-8)  # Add epsilon for stability\n",
        "    return mu_s, sigma_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### GRADIENT REGULARIZATION MODEL\n",
        "R_SCALAR = 0.05     # Approximation scalar 'r' controls the magnitude of the perturbation\n",
        "ALPHA_COEFF = 1.0   # Balance coefficient 'α' balance coefficient for the two loss components\n",
        "EPOCHS = 10         # Number of training epochs\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model = DeepfakeDetector_3(pretrained=True).to(device)\n",
        "pim = PerturbationInjectionModule().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n",
        "scaler = GradScaler()  # Initialize GradScaler for AMP\n",
        "\n",
        "logging.info(f\"Hyperparameters: epochs={EPOCHS}, lr={LEARNING_RATE}, r={R_SCALAR}, alpha={ALPHA_COEFF}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\")):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        \"\"\" Step 1: First Forward Pass for g1\n",
        "        This pass calculates the standard empirical loss and computes the gradients needed for both\n",
        "        the model update g1 and the perturbation calculation.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():  # Enable mixed precision\n",
        "            # Get shallow features\n",
        "            shallow_out = model.shallow_features_extractor(inputs)\n",
        "\n",
        "            # Compute the feature statistics (μ_s, σ_s) that represent image texture.\n",
        "            mu_s, sigma_s = compute_statistics(shallow_out)\n",
        "\n",
        "            \"\"\" Make sure statistics require gradients\n",
        "            This makes them leaf nodes, allowing us to get their gradients explicitly.\n",
        "            \"\"\"\n",
        "            mu_s = mu_s.detach().requires_grad_(True)\n",
        "            sigma_s = sigma_s.detach().requires_grad_(True)\n",
        "\n",
        "            # First forward pass\n",
        "            # Apply equation (6) to normalize the shallow features\n",
        "            x_norm = (shallow_out - mu_s) / (sigma_s + 1e-8)\n",
        "            reconstructed_shallow = x_norm * sigma_s + mu_s\n",
        "\n",
        "            # Calculate the standard empirical loss on the original unperturbed data\n",
        "            # Equation (1)\n",
        "            outputs_original = model.forward_from_shallow(reconstructed_shallow)\n",
        "            L_original = criterion(outputs_original, labels)\n",
        "\n",
        "        \"\"\"Scale loss and compute gradients\n",
        "        The first backward pass. It computes the gradient of the loss w.r.t all parameters in the graph.\n",
        "        retain_graph=True is crucial as it keeps the computation graph intact for the second backward pass\n",
        "        \"\"\"\n",
        "        scaler.scale(L_original).backward(retain_graph=True)\n",
        "\n",
        "        \"\"\"\n",
        "        Explicitly extract the gradients of the loss w.r.t the statistics (μ_s, σ_s).\n",
        "        These gradients indicate the direction of vulnerability of the model to texture changes.\n",
        "        \"\"\"\n",
        "        grad_mu_s, grad_sigma_s = torch.autograd.grad(\n",
        "            outputs=L_original,\n",
        "            inputs=[mu_s, sigma_s],\n",
        "            retain_graph=True,\n",
        "            create_graph=False\n",
        "        )\n",
        "\n",
        "        # Store gradients g1: the gradients of the model parameters after the first backward pass.\n",
        "        # g1: gradient of the original loss\n",
        "        g1 = [p.grad.clone() if p.grad is not None else torch.zeros_like(p)\n",
        "              for p in model.parameters()]\n",
        "\n",
        "        \"\"\"\n",
        "        This computes the actual perturbation vector based on the gradients found in the previous step.\n",
        "        \"\"\"\n",
        "        grad_vec = torch.cat([grad_mu_s.flatten(), grad_sigma_s.flatten()])\n",
        "        norm_grad = torch.norm(grad_vec)\n",
        "\n",
        "        # Step 2: Compute Perturbations by taking a small step 'r' in the direction of the gradients\n",
        "        # Equation (9)\n",
        "        if norm_grad > 1e-8:\n",
        "            delta_l = R_SCALAR * grad_vec / norm_grad\n",
        "        else:\n",
        "            delta_l = torch.zeros_like(grad_vec)\n",
        "\n",
        "        # Reshape perturbation vector back to the original statistics shape (mean, std)\n",
        "        delta_mu_s = delta_l[:mu_s.numel()].view_as(mu_s)\n",
        "        delta_sigma_s = delta_l[mu_s.numel():].view_as(sigma_s)\n",
        "\n",
        "        \"\"\" Step 3: Second Forward/Backward Pass for g2\n",
        "        This pass calculates the loss and gradient on the perturbed shallow features.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            # normalize the original shallow features\n",
        "            x_norm = (shallow_out.detach() - mu_s.detach()) / (sigma_s.detach() + 1e-8)\n",
        "            # use PIM to create the perturbed shallow features\n",
        "            perturbed_shallow_out = pim(x_norm, mu_s.detach(), sigma_s.detach(), delta_mu_s, delta_sigma_s)\n",
        "\n",
        "            \"\"\"\n",
        "            Calculate the loss on the perturbed data\n",
        "            Loss function from Equation 12\n",
        "            \"\"\"\n",
        "            outputs_perturbed = model.forward_from_shallow(perturbed_shallow_out)\n",
        "            L_perturbed = criterion(outputs_perturbed, labels)\n",
        "\n",
        "        \"\"\" Scale loss and compute gradients\n",
        "        This calculates the gradients of the model parameters w.r.t the perturbed loss.\n",
        "        \"\"\"\n",
        "        scaler.scale(L_perturbed).backward()\n",
        "\n",
        "        # Store gradients g2 from the second pass.\n",
        "        # g2: gradient of the perturbed loss\n",
        "        g2 = [p.grad.clone() if p.grad is not None else torch.zeros_like(p)\n",
        "              for p in model.parameters()]\n",
        "\n",
        "        # Step 4: Combine Gradients and Update Weights\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            for p_idx, p in enumerate(model.parameters()):\n",
        "                \"\"\"\n",
        "                set the final gradient for each parameter using the weighed combination of g1 and g2\n",
        "                this update rule encourages both accuracy (from g1) and robustness (from g2)\n",
        "                \"\"\"\n",
        "                combined_grad = (1 - ALPHA_COEFF) * g1[p_idx] + ALPHA_COEFF * g2[p_idx]\n",
        "                p.grad = combined_grad\n",
        "\n",
        "        # Update weights with scaler\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        \"\"\"\n",
        "        Calculate the combined loss\n",
        "        \"\"\"\n",
        "        combined_loss = (1 - ALPHA_COEFF) * L_original.item() + ALPHA_COEFF * L_perturbed.item()\n",
        "        running_loss += combined_loss\n",
        "\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            logging.info(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], Loss: {combined_loss:.4f}\")\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    logging.info(f\"\\nEpoch {epoch+1}\")\n",
        "    logging.info(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "        save_path = f'grad_reg/model_epoch_{epoch+1}_3.pth'\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        logging.info(f\"Model saved to '{save_path}'\")\n",
        "\n",
        "torch.save(model.state_dict(), 'grad_reg/final-full-3.pth')\n",
        "logging.info(\"Final model saved to 'final-full-3.pth'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perturbation Magnitude and Balance Coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ablation study parameters\n",
        "R_SCALARS = [0.05, 0.1]\n",
        "ALPHA_COEFFS = [1.0, 0.75]\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "results = []\n",
        "\n",
        "# Create a directory for ablation study outputs\n",
        "os.makedirs(\"ablation_study\", exist_ok=True)\n",
        "\n",
        "# Ablation study loop\n",
        "for r in R_SCALARS:\n",
        "    for alpha in ALPHA_COEFFS:\n",
        "        # Set up logging for this combination\n",
        "        log_dir = f\"ablation_study/r_{r}_alpha_{alpha}\"\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        logging.basicConfig(\n",
        "            filename=f\"{log_dir}/training.log\",\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "        )\n",
        "\n",
        "        # Log hyperparameters\n",
        "        logging.info(f\"Starting training with r={r}, alpha={alpha}\")\n",
        "        logging.info(f\"Hyperparameters: epochs={EPOCHS}, lr={LEARNING_RATE}, r={r}, alpha={alpha}\")\n",
        "\n",
        "        # Initialize model, optimizer, and other components\n",
        "        model = DeepfakeDetector(pretrained=True).to(device)\n",
        "        pim = PerturbationInjectionModule().to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n",
        "        scaler = GradScaler()\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\")):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with autocast():\n",
        "                    shallow_out = model.shallow_features_extractor(inputs)\n",
        "                    mu_s, sigma_s = compute_statistics(shallow_out)\n",
        "                    mu_s = mu_s.detach().requires_grad_(True)\n",
        "                    sigma_s = sigma_s.detach().requires_grad_(True)\n",
        "                    x_norm = (shallow_out - mu_s) / (sigma_s + 1e-8)\n",
        "                    reconstructed_shallow = x_norm * sigma_s + mu_s\n",
        "                    outputs_original = model.forward_from_shallow(reconstructed_shallow)\n",
        "                    L_original = criterion(outputs_original, labels)\n",
        "\n",
        "                scaler.scale(L_original).backward(retain_graph=True)\n",
        "                grad_mu_s, grad_sigma_s = torch.autograd.grad(\n",
        "                    outputs=L_original,\n",
        "                    inputs=[mu_s, sigma_s],\n",
        "                    retain_graph=True,\n",
        "                    create_graph=False\n",
        "                )\n",
        "                g1 = [p.grad.clone() if p.grad is not None else torch.zeros_like(p)\n",
        "                    for p in model.parameters()]\n",
        "\n",
        "                grad_vec = torch.cat([grad_mu_s.flatten(), grad_sigma_s.flatten()])\n",
        "                norm_grad = torch.norm(grad_vec)\n",
        "                if norm_grad > 1e-8:\n",
        "                    delta_l = r * grad_vec / norm_grad\n",
        "                else:\n",
        "                    delta_l = torch.zeros_like(grad_vec)\n",
        "\n",
        "                delta_mu_s = delta_l[:mu_s.numel()].view_as(mu_s)\n",
        "                delta_sigma_s = delta_l[mu_s.numel():].view_as(sigma_s)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    x_norm = (shallow_out.detach() - mu_s.detach()) / (sigma_s.detach() + 1e-8)\n",
        "                    perturbed_shallow_out = pim(x_norm, mu_s.detach(), sigma_s.detach(), delta_mu_s, delta_sigma_s)\n",
        "                    outputs_perturbed = model.forward_from_shallow(perturbed_shallow_out)\n",
        "                    L_perturbed = criterion(outputs_perturbed, labels)\n",
        "\n",
        "                scaler.scale(L_perturbed).backward()\n",
        "                g2 = [p.grad.clone() if p.grad is not None else torch.zeros_like(p)\n",
        "                    for p in model.parameters()]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with torch.no_grad():\n",
        "                    for p_idx, p in enumerate(model.parameters()):\n",
        "                        combined_grad = (1 - alpha) * g1[p_idx] + alpha * g2[p_idx]\n",
        "                        p.grad = combined_grad\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                combined_loss = (1 - alpha) * L_original.item() + alpha * L_perturbed.item()\n",
        "                running_loss += combined_loss\n",
        "\n",
        "                if (i + 1) % 50 == 0:\n",
        "                    logging.info(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], Loss: {combined_loss:.4f}\")\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            logging.info(f\"\\nEpoch {epoch+1}\")\n",
        "            logging.info(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "            if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "                save_path = f\"{log_dir}/model_epoch_{epoch+1}.pth\"\n",
        "                torch.save(model.state_dict(), save_path)\n",
        "                logging.info(f\"Model saved to '{save_path}'\")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = f\"{log_dir}/final-full.pth\"\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "logging.info(f\"Final model saved to '{final_model_path}'\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.load_state_dict(torch.load(final_model_path, map_location=device))\n",
        "model = model.to(device)\n",
        "avg_test_loss, test_auc, test_logloss, test_f1 = evaluate_model(model, test_loader, criterion, phase='test')\n",
        "\n",
        "# Log evaluation results\n",
        "logging.info(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "logging.info(f\"Test AUC: {test_auc:.4f}\")\n",
        "logging.info(f\"Test LogLoss: {test_logloss:.4f}\")\n",
        "logging.info(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "# Store results\n",
        "results.append({\n",
        "    'r': r,\n",
        "    'alpha': alpha,\n",
        "    'test_loss': avg_test_loss,\n",
        "    'test_auc': test_auc,\n",
        "    'test_logloss': test_logloss,\n",
        "    'test_f1': test_f1\n",
        "})\n",
        "\n",
        "# Print results table\n",
        "print(\"\\nAblation Study Results:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'r':<8}{'α':<8}{'Test Loss':<12}{'Test AUC':<12}{'Test LogLoss':<14}{'Test F1':<12}\")\n",
        "print(\"-\" * 80)\n",
        "for result in results:\n",
        "    print(f\"{result['r']:<8.2f}{result['alpha']:<8.2f}{result['test_loss']:<12.4f}{result['test_auc']:<12.4f}\"\n",
        "          f\"{result['test_logloss']:<14.4f}{result['test_f1']:<12.4f}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_model_attacks(\"ablation_study/r_0.1_alpha_0.75/final-full.pth\", \"r_0.1_alpha_0.75\") # model with gradient regularization\n",
        "evaluate_model_attacks(\"ablation_study/r_0.1_alpha_1.0//final-full.pth\", \"r_0.1_alpha_1.0\") # model with gradient regularization\n",
        "evaluate_model_attacks(\"ablation_study/r_0.05_alpha_0.75/final-full.pth\", \"r_0.05_alpha_0.75\") # model with gradient regularization\n",
        "evaluate_model_attacks(\"ablation_study/r_0.05_alpha_1.0//final-full.pth\", \"r_0.05_alpha_1.0\") # model with gradient regularization"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Eb6YUgKM5H_i",
        "g2bPw-5W4hJe",
        "Nm2TAq6B5UBI",
        "BoQYEe4V5j5E",
        "_6XKkV07tbZm",
        "gLi3I-3ltneX",
        "M19NYfKeVaY2",
        "siqE8FpPPm1q",
        "KKA4W3E-Qrii",
        "uYMjYd5jVy_Z",
        "PN9Ib804zYM0",
        "JVG8nFOi6Dbe",
        "vhLCflR28fxo",
        "EjiqGK4q42zP",
        "A05HEglkxQPy",
        "fQwVWofTAuJV",
        "fVd3Cgdy4xx8"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
